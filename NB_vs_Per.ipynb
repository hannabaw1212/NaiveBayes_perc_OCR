{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8cd41c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39bb8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train images and labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#loading training data\n",
    "trainingimages = pd.read_csv('trainingimages', sep = ' ', header = None, on_bad_lines='skip', engine='python')\n",
    "\n",
    "#dropping the last column to make the figure 28x28 as mentioned in the description\n",
    "trainingimages = trainingimages.iloc[:,:-1]\n",
    "\n",
    "#converting the data frame into 0 and 1 \n",
    "#  +,# = 1 else 0\n",
    "arr =  np.where(trainingimages.isnull(),0,1)\n",
    "\n",
    "#coverting array into dataframe again\n",
    "trainingimages = pd.DataFrame(arr)\n",
    "\n",
    "#separating each image from the dataframe on the basisi of its size (28x28)\n",
    "train = []\n",
    "for i in range(0, len(trainingimages), 28):\n",
    "    train.append(np.array(trainingimages.iloc[i:i+28,:]).reshape(28*28))\n",
    "    \n",
    "#making a dataframe for the train data\n",
    "train = pd.DataFrame(train)\n",
    "    \n",
    "    \n",
    "#training labels\n",
    "traininglabels =  pd.read_csv('traininglabels', header = None)\n",
    "traininglabels.columns = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930ed6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation images and labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#loading validation data\n",
    "validationimages = pd.read_csv('validationimages', sep = ' ', header = None, on_bad_lines='skip', engine='python')\n",
    "\n",
    "#dropping the last column to make the figure 28x28 as mentioned in the description\n",
    "validationimages = validationimages.iloc[:,:-1]\n",
    "\n",
    "#converting the data frame into 0 and 1 \n",
    "#  +,# = 1 else 0\n",
    "arr =  np.where(validationimages.isnull(),0,1)\n",
    "\n",
    "#coverting array into dataframe again\n",
    "validationimages = pd.DataFrame(arr)\n",
    "\n",
    "#separating each image from the dataframe on the basisi of its size (28x28)\n",
    "valid = []\n",
    "for i in range(0, len(validationimages), 28):\n",
    "    valid.append(np.array(validationimages.iloc[i:i+28,:]).reshape(28*28))\n",
    "    \n",
    "#making a dataframe for the train data\n",
    "valid = pd.DataFrame(valid)\n",
    "    \n",
    "    \n",
    "#validation labels\n",
    "validationlabels =  pd.read_csv('validationlabels', header = None)\n",
    "validationlabels.columns = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "703291b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test images and labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#loading test data\n",
    "testimages = pd.read_csv('testimages', sep = ' ', header = None, on_bad_lines='skip', engine='python')\n",
    "\n",
    "#dropping the last column to make the figure 28x28 as mentioned in the description\n",
    "testimages = testimages.iloc[:,:-1]\n",
    "\n",
    "#converting the data frame into 0 and 1 \n",
    "#  +,# = 1 else 0\n",
    "arr =  np.where(testimages.isnull(),0,1)\n",
    "\n",
    "#coverting array into dataframe again\n",
    "testimages = pd.DataFrame(arr)\n",
    "\n",
    "#separating each image from the dataframe on the basisi of its size (28x28)\n",
    "test = []\n",
    "for i in range(0, len(testimages), 28):\n",
    "    test.append(np.array(testimages.iloc[i:i+28,:]).reshape(28*28))\n",
    "    \n",
    "#making a dataframe for the train data\n",
    "test = pd.DataFrame(test)\n",
    "    \n",
    "    \n",
    "#validation labels\n",
    "testlabels =  pd.read_csv('testlabels', header = None)\n",
    "testlabels.columns = ['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6519b80",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e02f12",
   "metadata": {},
   "source": [
    "## Prior Probabilities Pr(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b268d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_prob = pd.DataFrame(traininglabels.value_counts()/len(traininglabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd96fb3",
   "metadata": {},
   "source": [
    "## Conditional Probabilities Pr (fij/C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fcd13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_train = train.copy()\n",
    "_train['label'] = traininglabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eecd9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list to grab a conditional probability table for each class\n",
    "post = []\n",
    "#looping through the classes\n",
    "for i in range(10):\n",
    "    #appending a CPT for each class\n",
    "     post.append(_train[_train['label']==i].drop('label', axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59e7250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate conditional probabilties with laplace smoothening\n",
    "def cond_prop_laplace(k):\n",
    "    # lists to grab conditional proabilities for class 0 and 1\n",
    "    probability_0 = []\n",
    "    probability_1 = []\n",
    "    #looping through all the classes (0-9)\n",
    "    for i in range(10):\n",
    "        #counting the '1' in the feature for a particular class\n",
    "        sum_ = post[i].sum()\n",
    "        #conditional probability of 1\n",
    "        probability_1.append(( sum_ + k)/(len(post[i])+k))\n",
    "        #conditional probability of 0\n",
    "        probability_0.append(1 - ( sum_ + k)/(len(post[i])+k))\n",
    "    #returning the conditional pribabilities\n",
    "    return probability_0, probability_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10877ed5",
   "metadata": {},
   "source": [
    "## Functon for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c1fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fucction for making prediction\n",
    "def prediction(data, prob_0_, prob_1_):\n",
    "    #list to grab predictions\n",
    "    pred = []\n",
    "    #loping through the data\n",
    "    for i in range(len(data)):\n",
    "        # grabbing the ith tuple\n",
    "        a =  data.iloc[i,:]\n",
    "        #grabbing probability for each class\n",
    "        probability =[]\n",
    "        # looping through all the 10 classes (0-9)\n",
    "        for j in range(10):\n",
    "            #grabbing conditionsl probablities\n",
    "            b = prob_0_[j]\n",
    "            c = prob_1_[j]\n",
    "            p = 1\n",
    "            #looping through all the features\n",
    "            for l in range(28*28):\n",
    "                #multiplying the conditional probabilities as per the condition\n",
    "                if a[l] ==0:\n",
    "                    p = p*b[l]\n",
    "                else:\n",
    "                    p = p*c[l]\n",
    "            # taking log and adding the probabilities as mentioned in the description\n",
    "            p = np.log(p)+ np.log(prior_prob.loc[j].values)\n",
    "            #appending probability od each class\n",
    "            probability.append(p)\n",
    "        #the class that shows maximum probability for any tuple becomes the prediction for that tuple\n",
    "        pred.append(np.argmax(probability))\n",
    "    # returing the predictions\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f41b3",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef87c9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.764"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k = 1\n",
    "prob_0, prob_1 = cond_prop_laplace(1)\n",
    "pred_ = prediction(valid, prob_0, prob_1)\n",
    "accuracy_score(validationlabels, pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83db49a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.759"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k = 2\n",
    "prob_0, prob_1 = cond_prop_laplace(2)\n",
    "pred_ = prediction(valid, prob_0, prob_1)\n",
    "accuracy_score(validationlabels, pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dae2a8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.749"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k = 3\n",
    "prob_0, prob_1 = cond_prop_laplace(3)\n",
    "pred_ = prediction(valid, prob_0, prob_1)\n",
    "accuracy_score(validationlabels, pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24072d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.744"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k = 4\n",
    "prob_0, prob_1 = cond_prop_laplace(4)\n",
    "pred_ = prediction(valid, prob_0, prob_1)\n",
    "accuracy_score(validationlabels, pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c168cb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.741"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k = 5\n",
    "prob_0, prob_1 = cond_prop_laplace(5)\n",
    "pred_ = prediction(valid, prob_0, prob_1)\n",
    "accuracy_score(validationlabels, pred_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d368f83",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "273e5691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.718"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As the best validation accuracy was achived at k = 1\n",
    "# k = 1\n",
    "prob_0, prob_1 = cond_prop_laplace(1)\n",
    "pred_ = prediction(test, prob_0, prob_1)\n",
    "accuracy_score(testlabels, pred_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb2ee8a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0153c217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.798"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As the best validation accuracy was achived at k = 1\n",
    "# k = 1\n",
    "prob_0, prob_1 = cond_prop_laplace(1)\n",
    "pred_ = prediction(train, prob_0, prob_1)\n",
    "accuracy_score(traininglabels, pred_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c5b082",
   "metadata": {},
   "source": [
    "Recorded errors are not zero because if the train accuracy becomes 100% then it means that the model has overfit. \n",
    "\n",
    "The recorded accuracy for training subset is better than that of the test subset.The reason being, train data is already seen by the model during training. However, test data is unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf362c2",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b0dc676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding the labels for perceptron training\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(traininglabels)\n",
    "traininglabels_enc = enc.transform(traininglabels).toarray()\n",
    "#converting labels to -1 and 1\n",
    "traininglabels_enc = np.where(traininglabels_enc==0,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "431e68f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation function\n",
    "def activation_func(value):    #sigmoid\n",
    "    return (1/(1+np.exp(-value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48422252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for perceptron training\n",
    "def perceptron(in_data,labels, epochs):\n",
    "    #list to grab weight vector for each class\n",
    "    w = []\n",
    "    #looping through all the classes (0-9)\n",
    "    for i in range(traininglabels_enc.shape[1]):\n",
    "        #converting data into array\n",
    "        X=np.array(in_data)\n",
    "        y=np.array(labels[:,i])\n",
    "        #initialization of weight vector for each class as zero\n",
    "        weights=np.zeros(X.shape[1])\n",
    "        original=weights\n",
    "        #looping through the epochs\n",
    "        for epoch in range(epochs):\n",
    "            #looping through the tuples\n",
    "            for key in range(X.shape[0]):\n",
    "                #calling activation function\n",
    "                a=activation_func(np.matmul(np.transpose(weights),X[key]))\n",
    "                # 1 or -1 classification for each class\n",
    "                if a>=0.5:\n",
    "                    yn=1\n",
    "                else:\n",
    "                    yn=-1\n",
    "                #weights updation\n",
    "                weights=weights+(yn-y[key])*X[key] \n",
    "        #appending weight of each class to the list of weights\n",
    "        w.append(weights)\n",
    "    #returning weight vector\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e187947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_test(in_data,weights):\n",
    "    #making the data as array\n",
    "    X=np.array(in_data)\n",
    "    #list to grab predictions\n",
    "    y_pred = []\n",
    "    #looping through the data\n",
    "    for i in range(len(in_data)):\n",
    "        #list to grab scores for each tuple\n",
    "        y_ = []\n",
    "        #ith tuple\n",
    "        x = X[i]\n",
    "        #looping through the classes (0 - 9)\n",
    "        for j in range(len(weights)):\n",
    "            #grabbind the weight vector for the said class\n",
    "            w_ = weights[j]\n",
    "            # calculating the score\n",
    "            score = np.abs(activation_func(w_*x)).sum()\n",
    "            #appending the score\n",
    "            y_.append(score)\n",
    "        #appending the class with the maximum score\n",
    "        y_pred.append(np.argmax(y_))\n",
    "    #returning the predictions\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43784c56",
   "metadata": {},
   "source": [
    "## When epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83c7cb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.0946\n",
      "Test Accuracy: 0.09\n"
     ]
    }
   ],
   "source": [
    "list_of_weights = perceptron(train,traininglabels_enc, 3)\n",
    "\n",
    "# train accuracy\n",
    "prediction =  perceptron_test(train,list_of_weights)\n",
    "print(f'Train Accuracy: {accuracy_score(traininglabels, prediction)}')\n",
    "\n",
    "# test accuracy\n",
    "prediction =  perceptron_test(test,list_of_weights)\n",
    "accuracy_score(testlabels, prediction)\n",
    "print(f'Test Accuracy: {accuracy_score(testlabels, prediction)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ed402",
   "metadata": {},
   "source": [
    "## When epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f218042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.0912\n",
      "Test Accuracy: 0.081\n"
     ]
    }
   ],
   "source": [
    "list_of_weights = perceptron(train,traininglabels_enc, 1)\n",
    "\n",
    "# train accuracy\n",
    "prediction =  perceptron_test(train,list_of_weights)\n",
    "print(f'Train Accuracy: {accuracy_score(traininglabels, prediction)}')\n",
    "\n",
    "# test accuracy\n",
    "prediction =  perceptron_test(test,list_of_weights)\n",
    "accuracy_score(testlabels, prediction)\n",
    "print(f'Test Accuracy: {accuracy_score(testlabels, prediction)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04078f29",
   "metadata": {},
   "source": [
    "## When epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dce86eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.0938\n",
      "Test Accuracy: 0.088\n"
     ]
    }
   ],
   "source": [
    "list_of_weights = perceptron(train,traininglabels_enc, 2)\n",
    "\n",
    "# train accuracy\n",
    "prediction =  perceptron_test(train,list_of_weights)\n",
    "print(f'Train Accuracy: {accuracy_score(traininglabels, prediction)}')\n",
    "\n",
    "# test accuracy\n",
    "prediction =  perceptron_test(test,list_of_weights)\n",
    "accuracy_score(testlabels, prediction)\n",
    "print(f'Test Accuracy: {accuracy_score(testlabels, prediction)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c6161",
   "metadata": {},
   "source": [
    "## When epoch = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa349998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.095\n",
      "Test Accuracy: 0.09\n"
     ]
    }
   ],
   "source": [
    "list_of_weights = perceptron(train,traininglabels_enc, 4)\n",
    "\n",
    "# train accuracy\n",
    "prediction =  perceptron_test(train,list_of_weights)\n",
    "print(f'Train Accuracy: {accuracy_score(traininglabels, prediction)}')\n",
    "\n",
    "# test accuracy\n",
    "prediction =  perceptron_test(test,list_of_weights)\n",
    "accuracy_score(testlabels, prediction)\n",
    "print(f'Test Accuracy: {accuracy_score(testlabels, prediction)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef37992d",
   "metadata": {},
   "source": [
    "## When epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11b90fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.095\n",
      "Test Accuracy: 0.09\n"
     ]
    }
   ],
   "source": [
    "list_of_weights = perceptron(train,traininglabels_enc, 5)\n",
    "\n",
    "# train accuracy\n",
    "prediction =  perceptron_test(train,list_of_weights)\n",
    "print(f'Train Accuracy: {accuracy_score(traininglabels, prediction)}')\n",
    "\n",
    "# test accuracy\n",
    "prediction =  perceptron_test(test,list_of_weights)\n",
    "accuracy_score(testlabels, prediction)\n",
    "print(f'Test Accuracy: {accuracy_score(testlabels, prediction)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83fc07b",
   "metadata": {},
   "source": [
    "# Perceptron vs Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e853d",
   "metadata": {},
   "source": [
    "**Perceptron** laid the basis of the cutting edge **deep learning models** which are able to model any non linearities in the data with the help of complex activation functions and deep stacked layers. However, perceptron it self is not a very efficient way of solving a multi class classification. Additioally, it may require large number of epochs (computational power) to update the weights. We can see in this example that the perceptron performed poorly. *However, the accuracy, was increasing with the increase of number of epochs yet the results are not acceptable*.\n",
    "\n",
    "**Naive Bayes** is a conventional and very powerful algorithm that is able to perform exceptional on many business problems. It is extensively used in image classification, natural language processing, etc. *In this example, we can see that Naive Bayes performed well and was able to achieve accetable accuracy on validation and the test dataset.*\n",
    "\n",
    "\n",
    "However, it must be noted that while solving any **image related problem, information in spatial locations play a vital part. Neither Naive Bayes, nor Perceptron has the capability to keep the information related to spatial locations. CNN is most suited for solving such image related problems**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6f0f18e929f98629b0ad9187bbe400905e62f038b2875773787324ab39080a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
